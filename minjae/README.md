## 머신러닝

### 01. 파이썬 기반의 머신러닝과 생태계 이해
   1) 머신러닝 : 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭.
   2) 머신러닝 분류
      - 지도학습 : 분류, 회귀, 추천시스템, 시각/음성 감지/인지, 텍스트 분석, NLP
      - 비지도학습 : 클러스터링, 차원 축소, 강화학습
   3) 주요 패키지 : numpy, pandas, matplotlib, seaborn, sklearn 등
   4) numpy : 선형대수 기반 프로그램 쉽게 만들 수 있도록 지원, 행렬의 연산

```python
import numpy as np
```
   5) pandas : DataFrame을 통해 numpy보다 편한 데이터 핸들링 가능
```python
import pandas as pd
```

### 02. 사이킷런으로 시작하는 머신러닝
- 사이킷런 : 머신러닝 라이브러리 중 가장 많이 사용, 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크, API 제공

1. 교차 검증 : 데이터의 편증을 막기 위해 여러 세트로 구성된 학습, 테스트 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것 -> 고정된 데이터셋으로 평가를 하다보면 테스트 데이터에만 최적의 성능을 발휘할 수 있음

   - K 폴드 교차검증
   - Stratified K 폴드 : K 폴드의 불균형한 레이블 분포 문제를 해결  ex) 데이터셋을 나누었을 때 각 레이블의 비율이 일정하지 않으면 문제가 생김.
   - cross_val_score() : 교차검증을 더 간편하게 수행하는 API.
   - GridSearchCV : 교차검증과 최적 하이퍼 파라미터(세팅 값) 튜닝을 한 번에 가능.


2. 전처리
   1. 데이터 인코딩
      - 레이블 인코딩 : 카테고리 feature들을 0,1,2,3 등의 숫자로 변환하는 것
      - 원-핫인코딩 : feature 값의 유형에 따라 새로운 feature를 추가해 해당하는 컬럼에 1을 표시, 나머지는 0

   2. 피처 스케일링
      - 표준화 : feature 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것
      -> 서로 다른 feature의 크기를 통일하기 위해 크기를 변환해주는 작업 
        - StandardScaler : 일반적인 표준화 
        - MinMaxScaler : 데이터의 분포가 가우시안 분포가 아닌 경우
      - 정규화(Nomalizer) : 사이킷런 모듈은 선형대수에서의 정규화 개념, 개별 벡터의 크기를 맞추기 위해 변환, 개별 벡터를 모든 피처 벡터 크기로 나눠줌 


### 03. 평가

- 분류의 성능 평가 지표 : 정확도, 오차행렬, 정밀도, 재현율, F1 스코어, ROC AUC

1. 정확도(Accuracy) : 실제 데이터에서 예측 데이터가 얼마나 같은지 판단하는 지표
    
   - 단순한 알고리즘으로 예측하더라도 정확도가 비교적 높게 측정

   - 정확도를 평가지표로 사용할 때는 매우 신중해야 함.

2. 오차행렬(Confusion Matrix) :  이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표


3. 정밀도(Precision) / 재현율(Recall) : : Positive 데이터 세트의 예특 성능에 좀 더 초점을 맞춘 평가 지표
   - 정밀도 : TP / (FP+TP) 
     - 예측을 positive로 한 대상 중에 예측과 실제값이 positive로 일치한 데이터의 비율
     - Positive 예측 성능을 더욱 정밀하게 측정하기 위한 평가 지표 = 양성 예측도
     - 재현율이 중요한 경우 : positive를 negative로 잘못 판단하면 큰 영향이 발생하는 경우
   - 재현율(민감도, TPR) : TP / (FN+TP)
     - 실제 값이 positive인 대상 중에 예측과 실제 값이 positive로 일치한 데이터의 비율
     - 정밀도가 중요한 경우 : negative를 positive로 잘못 판단하면 큰 영향이 발생하는 경우

4. F1 스코어 : 정밀도와 재현율을 결합한 지표
   - 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가짐.


6. ROC AUC 
   - ROC 곡선 : FPR(X축)이 변할 때 TPR(Y축)이 어떻게 변하는지 나타내는 곡선
     - TPR : 재현율 = 민감도, positive가 정확히 예측되어야 하는 수준
     - TNR : 특이성, 민감도에 대응하는 지표, negative가 정확히 예측되어야 하는 수준
     - **FPR = FP / (FP+TN) = 1 - TNR**
   - AUC ( Area Under Curve) : ROC 곡선 밑의 면적을 구한 것
     - 1에 가까울수록 좋은 수치


### 04. 분류

1. 결정트리 (Decision Tree): 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것

    - 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 좌우함.
    - 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음 => 과적합 문제 발생할 수 있음
    - 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 만흔 데이터 세트가 해당 분류에 속할 수 있도록 규칙이 정해져야 함 => 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 중요
    
    - 장점 : 쉽다, 직관적이다, 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않다.
    - 단점 : 과적합으로 알고리즘 성능이 떨어진다. 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요.

![dt](https://user-images.githubusercontent.com/84848848/148776039-975a3ab1-fda2-4262-bbec-b9f70d1f48f8.png)
    
2. 앙상블 (Ensemble) : 여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법
   - 학습 유형
       - 보팅(Voting) : 서로 다른 알고리즘을 가진 분류기를 결합
       - 배깅(Bagging) : 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 다르게 하여 학습을 수행해 보팅을 수행하는 것, 대표적으로 랜덤 포레스트.
       - 부스팅(Boostion) : 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식
![e4](https://user-images.githubusercontent.com/84848848/148785998-769b9f67-b738-47b4-a5f6-bf1e225dedc6.png)
     
3. 랜덤 포레스트 (Random Forest) : 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 함.

4. GMB (Gradient Boosting Machine) : 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘인 에이다 부스트와 비슷하지만 가중치 업데이트를 경사하강법을 이용한다.

   - 오류값 = 실제값 - 예측값 <= 오류식을 최소화하는 방향으로 반복적으로 가중치 업데이트
   - GBM은 CART 기반의 다른 알고리즘과 마찬가지로 회귀도 가능
![g](https://user-images.githubusercontent.com/84848848/148872238-d5da252a-1bb0-4404-b94d-526687aa4491.png)

5. XGBoost ( eXtra Gradient Boost): GBM에 기반하지만, 병렬 CPU 환경에서 병력 학습이 가능해 기존 GBM보다 빠르게 학습을 완료한다.
   - 분류에 있어서 일반적으로 다른 머신러닝보다 뛰어난 예측 성능을 나타냄
   - 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나
   - 장점
       - 뛰어난 예측 성능
       - GBM 대비 빠른 수행 시간
       - 과적합 규제
       - Tree pruning(나무 가지치기)
       - 자체 내장된 교차 검증
       - 결손값 자체 처리

6. LightGBM : 리프 중심 트리 분할 방식 사용. 트리의 균형을 맞추지 않고, 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성됨.=> 예측 오류 손실 최소화
   - 장점
       - 더 빠른 학습과 예측 수행 시간
       - 더 작은 메모리 사용량
       - 카테고리형 피처의 자동 변환과 최적 분할(원-핫 인코딩 등을 사용하지 않고도 피처를 최적으로 변환하고 이에 따른 노드 분할 수행)
![l](https://user-images.githubusercontent.com/84848848/148873560-c26a84aa-9c99-481f-8528-dc82f667cd88.png)


7. 스태킹 앙상블 : 개별적인 여러 알고리즘을 결합해 예측 결과를 도출, 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측 수행.
![s](https://user-images.githubusercontent.com/84848848/148875171-59d15731-6dba-4701-876d-664e7e8703d6.png)

